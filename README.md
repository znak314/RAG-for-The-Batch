# RAG-for-The-Batch
Multimodal Retrieval-Augmented  Generation (RAG) System that retrieves relevant news articles from The Batch - https://www.deeplearning.ai/the-batch/

**Important:** check Task report.pdf file. You can also see the [Demo](https://drive.google.com/file/d/1nj5u7Mmzbsg2KEhs15PwA8X92NszZDnE/view?usp=drive_link)

## ğŸš€ Technology Stack

- **Python 3.13**
- **FAISS** â€” fast similarity search
- **HuggingFace Sentence-Transformers** (`all-MiniLM-L6-v2`)
- **CrossEncoder** (`ms-marco-MiniLM-L-6-v2`)
- **GPT-4o** â€” for answer generation (via OpenAI API)
- **Streamlit** â€” interactive web UI
- **BeautifulSoup** + **requests** â€” for data ingestion
- **tqdm**, **pandas**, **numpy** â€” utilities and data handling

## Project Structure
```plaintext
RAG-for--Batch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_scraper/
â”‚   â”‚   â”œâ”€â”€ articles_scraper.py   # Utilities for scraping articles
â”‚   â”‚   â”œâ”€â”€ scrap.py              # Main scraping script
â”‚   â”‚
â”‚   â”œâ”€â”€ indexer.py                # Build / update FAISS vector index
â”‚   â”œâ”€â”€ app.py                    # Streamlit UI application
â”‚   â”œâ”€â”€ evaluator.py              # Evaluation pipeline (Hit@K, MRR)
â”‚
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Project dependencies
```

## How to setup?
### 1. Clone the repository
```
git clone https://github.com/znak314/RAG-for-The-Batch.git
cd RAG-for-The-Batch
```

### 2ï¸. Create a virtual environment and install dependencies

```bash
python -m venv venv
# On Linux / macOS:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

pip install -r requirements.txt
```

### 3. Add articles
You have two options to prepare the articles dataset:

#### Option 1 â€” Automatically scrape articles (recommended if you want latest data)

You can run the provided script with customizable arguments:

```bash
python src/data_scraper/scrap.py --output all_articles.json
```

#### Option 2 â€” Download prepared dataset
If you prefer, you can download a pre-scraped dataset and place it in the project root:

ğŸ‘‰ [Download all_articles.json from Google Drive](https://drive.google.com/file/d/1z-iGglYMNJc8Nv8pYC6x5Q4L5enOmkt6/view?usp=drive_link)

### 4ï¸. Build vector index

Once you have prepared the articles (`all_articles.json`), you need to build the FAISS vector index.  
You have two options:

---

#### Option 1 â€” Build index from articles (recommended)

Run the provided indexer script:

```bash
python src/indexer.py
```

#### Option 2 â€” Download prepared vectorstore
If you want to skip building the index, you can download a ready-to-use vectorstore:

ğŸ‘‰ [Download vectorstore from Google Drive](https://drive.google.com/drive/folders/1t5JhUMtWLUxTqoFB7d-0lp-hbgMo_Ofw?usp=drive_link)

Extract the contents into the /vectorstore/ folder.

### 5. Create .env and add ChatGPT API key
In order to use **GPT-4o** for answer generation, you need to provide your OpenAI API key.
```plaintext
OPENAI_API_KEY=your_openai_api_key_here
```

After all, you project should look like: 

```plaintext
RAG-for--Batch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_scraper/
â”‚   â”‚   â”œâ”€â”€ articles_scraper.py   # Utilities for scraping articles
â”‚   â”‚   â”œâ”€â”€ scrap.py              # Main scraping script
â”‚   â”‚
â”‚   â”œâ”€â”€ indexer.py                # Build / update FAISS vector index
â”‚   â”œâ”€â”€ app.py                    # Streamlit UI application
â”‚   â”œâ”€â”€ evaluator.py              # Evaluation pipeline (Hit@K, MRR)
â”‚
â”œâ”€â”€ all_articles.json             # Full scraped articles dataset (JSON)
â”œâ”€â”€ vectorstores/                 # FAISS index + metadata
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Project dependencies
â”œâ”€â”€ .env                          # Environmental variables
```

### 6. Run streamlit project
After completing the previous steps, you can now launch the Streamlit application:

```bash
streamlit run src/app.py
```

### 7. (Optional) Run evaluation script
Download test data from [Google drive](https://drive.google.com/file/d/1tZGZvZk3ZL7rJ7mrQwBOhqh7kyHNtTtQ/view?usp=drive_link)

Run the evaluator:

```bash
python src/evaluator.py
```
